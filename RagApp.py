from llama_cpp import Llama
from langchain.text_splitter import RecursiveCharacterTextSplitter
# Removed Chroma dependency to avoid conflicts
from langchain_community.document_loaders import TextLoader
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np
import gradio as gr
import os

# You can use this section to suppress warnings generated by your code:
def warn(*args, **kwargs):
    pass
import warnings
warnings.warn = warn
warnings.filterwarnings('ignore')

# Memory table to store conversation history
conversation_memory = []

## Local LLM
def get_llm():
    try:
        llm = Llama(
            model_path="C:/Users/User/AppData/Local/llama.cpp/TheBloke_Mistral-7B-Instruct-v0.2-GGUF_mistral-7b-instruct-v0.2.Q4_K_M.gguf",
            n_threads=12,
            n_ctx=1024,
            verbose=False
        )
        return llm
    except Exception as e:
        print(f"Error loading model: {e}")
        return None

## Document loader for text file
def document_loader():
    if os.path.exists("doc.txt"):
        loader = TextLoader("doc.txt", encoding="utf-8")
        loaded_document = loader.load()
        return loaded_document
    else:
        print("doc.txt file not found!")
        return []

## Text splitter
def text_splitter(data):
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200,
        length_function=len,
    )
    chunks = text_splitter.split_documents(data)
    return chunks

## Simple TF-IDF based retrieval system
class SimpleTFIDFRetriever:
    def __init__(self, documents):
        self.documents = documents
        self.texts = [doc.page_content for doc in documents]
        self.vectorizer = TfidfVectorizer(stop_words='english', max_features=1000)
        self.tfidf_matrix = self.vectorizer.fit_transform(self.texts)
    
    def get_relevant_documents(self, query, k=3):
        query_vector = self.vectorizer.transform([query])
        similarities = cosine_similarity(query_vector, self.tfidf_matrix).flatten()
        top_indices = similarities.argsort()[-k:][::-1]
        return [self.documents[i] for i in top_indices if similarities[i] > 0]

## Retriever
def get_retriever():
    splits = document_loader()
    if not splits:
        return None
    chunks = text_splitter(splits)
    retriever = SimpleTFIDFRetriever(chunks)
    return retriever

## Load prompt from file
def load_prompt():
    if os.path.exists("prompt.txt"):
        with open("prompt.txt", "r", encoding="utf-8") as f:
            return f.read().strip()
    else:
        return "Provide an answer based on the document and conversation context."

## Format conversation history
def format_conversation_history():
    if not conversation_memory:
        return ""
    
    history_text = "\n\nPrevious conversation:\n"
    for entry in conversation_memory[-5:]:  # Keep last 5 exchanges
        history_text += f"User: {entry['user']}\n"
        history_text += f"AI: {entry['ai']}\n"
    history_text += "\nCurrent question:\n"
    return history_text

## RAG with local LLM and memory
def rag_with_memory(query):
    global conversation_memory
    
    llm = get_llm()
    if not llm:
        return "Error: Could not load the local LLM model."
    
    retriever_obj = get_retriever()
    if not retriever_obj:
        return "Error: Could not load the document. Make sure doc.txt exists."
    
    # Get relevant documents
    relevant_docs = retriever_obj.get_relevant_documents(query)
    context = "\n".join([doc.page_content for doc in relevant_docs])
    
    # Load system prompt
    system_prompt = load_prompt()
    
    # Format conversation history
    conversation_context = format_conversation_history()
    
    # Construct the full prompt
    full_prompt = f"{system_prompt}\n\nDocument context:\n{context}{conversation_context}User: {query}\nAI: "
    
    try:
        # Generate response using local LLM
        response_tokens = []
        for token in llm(full_prompt, max_tokens=300, echo=False, stream=True):
            response_tokens.append(token["choices"][0]["text"])
        
        response = "".join(response_tokens).strip()
        
        # Save to memory
        conversation_memory.append({
            "user": query,
            "ai": response
        })
        
        # Keep only last 10 conversations to manage memory
        if len(conversation_memory) > 10:
            conversation_memory = conversation_memory[-10:]
        
        return response
        
    except Exception as e:
        return f"Error generating response: {e}"

## Format conversation history for display
def get_conversation_history():
    if not conversation_memory:
        return "No conversation history yet."
    
    history_display = ""
    for i, entry in enumerate(conversation_memory, 1):
        history_display += f"**Exchange {i}:**\n"
        history_display += f"üë§ **User:** {entry['user']}\n"
        history_display += f"ü§ñ **AI:** {entry['ai']}\n\n"
    
    return history_display

## Clear memory function
def clear_memory():
    global conversation_memory
    conversation_memory = []
    return "Memory cleared!", ""

# Create Gradio interface with memory display
with gr.Blocks(title="Local RAG with Memory") as rag_application:
    gr.Markdown("# üß† Local RAG Application with Memory")
    gr.Markdown("Ask questions about your document (doc.txt). The AI remembers previous conversations!")
    
    with gr.Row():
        with gr.Column(scale=2):
            query_input = gr.Textbox(
                label="üí¨ Ask a question", 
                lines=3, 
                placeholder="Type your question here..."
            )
            
            with gr.Row():
                submit_btn = gr.Button("üöÄ Submit", variant="primary")
                clear_btn = gr.Button("üóëÔ∏è Clear Memory", variant="secondary")
            
            answer_output = gr.Textbox(
                label="ü§ñ AI Response", 
                lines=8, 
                interactive=False
            )
        
        with gr.Column(scale=1):
            memory_display = gr.Textbox(
                label="üìö Conversation History", 
                lines=15, 
                interactive=False
            )
    
    # Event handlers
    submit_btn.click(
        fn=lambda query: (rag_with_memory(query), get_conversation_history()),
        inputs=[query_input],
        outputs=[answer_output, memory_display]
    )
    
    query_input.submit(
        fn=lambda query: (rag_with_memory(query), get_conversation_history()),
        inputs=[query_input],
        outputs=[answer_output, memory_display]
    )
    
    clear_btn.click(
        fn=clear_memory,
        outputs=[answer_output, memory_display]
    )
    
    # Load conversation history on startup
    rag_application.load(
        fn=get_conversation_history,
        outputs=[memory_display]
    )

# Launch the app
if __name__ == "__main__":
    print("Starting RAG application...")
    print("Make sure you have:")
    print("1. doc.txt - your document file")
    print("2. prompt.txt - your system prompt")
    print("3. Local LLM model at the specified path")
    rag_application.launch(server_name="127.0.0.1", server_port=7860, share=True)